{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcnn_utils import get_images_target, get_object_detection_model, decode_prediction\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GPU if one exists.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using: \", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 17 seconds\n",
    "\n",
    "# Get data stores in seal counter directory\n",
    "data_path = \"../../../seal_detector/Data\"\n",
    "\n",
    "# Load Data\n",
    "train_img_data = np.load(f\"{data_path}/train_images.npy\", allow_pickle=True)\n",
    "train_bb_data = np.load(f\"{data_path}/train_bb_data.npy\", allow_pickle=True)\n",
    "\n",
    "val_img_data  = np.load(f\"{data_path}/val_images.npy\", allow_pickle=True)\n",
    "val_bb_data = np.load(f\"{data_path}/val_bb_data.npy\", allow_pickle=True)\n",
    "\n",
    "test_img_data  = np.load(f\"{data_path}/test_images.npy\", allow_pickle=True)\n",
    "test_bb_data = np.load(f\"{data_path}/test_bb_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../..//Models/rcnn_resnet_v1_unfrozen_transformations_step_50_with_backbone_weights_50\"\n",
    "\n",
    "# Load model\n",
    "model = get_object_detection_model(1)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Putting model on GPU\n",
    "_ = (\n",
    "    model\n",
    "    .to(device)\n",
    "    .eval()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sub-images and bounding box data for training data\n",
    "training_sub_images, training_target = get_images_target(train_img_data, train_bb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sub_images, val_target = get_images_target(val_img_data, val_bb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_images, test_target = get_images_target(test_img_data, test_bb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_count(pred):\n",
    "    return len(pred[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(sub_images, targets, model, score_threshold=.9):\n",
    "    trans = transforms.Compose([transforms.ToTensor()])\n",
    "    metric = MeanAveragePrecision(extended_summary=True)\n",
    "    predictions = []\n",
    "    predicted_counts = []\n",
    "    actual_count = []\n",
    "\n",
    "    for i in tqdm(range(len(sub_images))):\n",
    "        image = sub_images[i]\n",
    "        \n",
    "        # Transform image to be passed to the model\n",
    "        image = (\n",
    "            trans(image)\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "            )\n",
    "        \n",
    "        # Make prediction on sub_image\n",
    "        raw_prediction = model(image)[0]\n",
    "        boxes, labels, scores= decode_prediction(raw_prediction, score_threshold)\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"boxes\": boxes, \n",
    "                \"labels\":labels, \n",
    "                \"scores\":scores\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Record predicted and actual counts\n",
    "        predicted_counts.append(len(boxes))\n",
    "        actual_count.append(len(targets[i][\"boxes\"]))\n",
    "    \n",
    "    # Numerical Count Difference\n",
    "    predicted_counts = np.array(predicted_counts)\n",
    "    actual_count = np.array(actual_count)\n",
    "    count_dif = abs(predicted_counts - actual_count)\n",
    "    over_counts = (predicted_counts > actual_count)\n",
    "    under_counts = (predicted_counts < actual_count)\n",
    "\n",
    "    # Print count metrics\n",
    "    print(f\"Totals Seals: {actual_count.sum()} Total Seals Predicted: {predicted_counts.sum()} Total Error: {count_dif.sum()}\")\n",
    "    print(f\"Average Error per sub-image: {count_dif.mean()}\")\n",
    "    print(f\"Average Percent Error per sub-iomage: {(count_dif / actual_count).mean()}\")\n",
    "    print(f\"Sub-images Overcounted: {over_counts.sum()}, Average Overcount Diff:{count_dif[over_counts].mean()}\")\n",
    "    print(f\"Sub-images Undercounted: {under_counts.sum()}, Average Overcount Diff:{count_dif[under_counts].mean()}\")\n",
    "    \n",
    "    # Calculate MAP\n",
    "    metric.update(predictions, targets)\n",
    "    result = metric.compute()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_map(training_sub_images, training_target, model, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_map(val_sub_images, val_target, model, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_map(test_sub_images, test_target, model, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
