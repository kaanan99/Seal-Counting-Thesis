{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights, fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision import ops\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Connect to the GPU if one exists.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using: \", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_model(model_path, version, num_classes = 2, feature_extraction = True, device = device):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        num_classes: int\n",
    "            Number of classes to predict. Must include the \n",
    "            background which is class 0 by definition!\n",
    "        feature_extraction: bool\n",
    "            Flag indicating whether to freeze the pre-trained \n",
    "            weights. If set to True the pre-trained weights will be  \n",
    "            frozen and not be updated during.\n",
    "    Returns\n",
    "        model: FasterRCNN\n",
    "    \"\"\"\n",
    "    if version == 2:\n",
    "        model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "    elif version == 1:\n",
    "        model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)    \n",
    "    # If True, the pre-trained weights will be frozen.\n",
    "    if feature_extraction == True:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False    \n",
    "    # Replace the original 91 class top layer with a new layer\n",
    "    # tailored for num_classes.\n",
    "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def detach_pred(prediction):\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    scores = prediction[\"scores\"]\n",
    "    labels = prediction[\"labels\"]    \n",
    "    return {\"boxes\":boxes.detach().cpu(), \"labels\":labels.detach().cpu(), \"scores\":scores.detach().cpu()}\n",
    "\n",
    "def predict(model, image, device=\"cpu\", transform = transforms.Compose([transforms.ToTensor()])):\n",
    "    image = transform(np.array(image)).unsqueeze(0).type(torch.FloatTensor).to(device)\n",
    "    pred = detach_pred(model(image)[0])\n",
    "    return pred\n",
    "\n",
    "def decode_prediction(prediction, \n",
    "                      score_threshold = 0.8, \n",
    "                      nms_iou_threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        prediction: dict\n",
    "        score_threshold: float\n",
    "        nms_iou_threshold: float\n",
    "    Returns\n",
    "        prediction: tuple\n",
    "    \"\"\"\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    scores = prediction[\"scores\"]\n",
    "    labels = prediction[\"labels\"]    \n",
    "    # Remove any low-score predictions.\n",
    "    if score_threshold is not None:\n",
    "        want = scores > score_threshold\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]    \n",
    "    # Remove any overlapping bounding boxes using NMS.\n",
    "    if nms_iou_threshold is not None:\n",
    "        want = ops.nms(boxes = boxes, scores = scores, iou_threshold = nms_iou_threshold)\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]    \n",
    "    return {\"boxes\":boxes, \"labels\":labels, \"scores\":scores}\n",
    "\n",
    "\n",
    "\n",
    "def get_seal_preds(sub_images, rcnn, device = device):\n",
    "    preds = []\n",
    "    for image in tqdm(sub_images):\n",
    "        pred = predict(rcnn, image[0], device)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def get_data(img_data, bb_data):\n",
    "    seal_images = []\n",
    "    seal_target = []\n",
    "    for i in range(len(bb_data)):\n",
    "        if bb_data[i] is not None:\n",
    "            seal_box = []\n",
    "            seal_label = []\n",
    "            for row in range(bb_data[i].shape[0]):\n",
    "                the_row = bb_data[i].iloc[row, :]\n",
    "                new_box = []\n",
    "                if int(the_row[\"xmin\"]) != int(the_row[\"xmax\"]) and int(the_row[\"ymin\"]) != int(the_row[\"ymax\"]):\n",
    "                    new_box.append(int(the_row[\"xmin\"]))\n",
    "                    new_box.append(int(the_row[\"ymin\"]))\n",
    "                    new_box.append(int(the_row[\"xmax\"]))\n",
    "                    new_box.append(int(the_row[\"ymax\"]))\n",
    "                    seal_box.append(new_box)\n",
    "                    seal_label.append(1)\n",
    "            if len(seal_box) > 0:\n",
    "                seal_images.append(img_data[i])\n",
    "                new_target = {}\n",
    "                new_target[\"boxes\"] = torch.tensor(seal_box)\n",
    "                new_target[\"labels\"] = torch.tensor(seal_label)\n",
    "                seal_target.append(new_target)\n",
    "    seal_images = np.array(seal_images)\n",
    "    return seal_images, seal_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data\n",
    "train_img_data = np.load(r\"C:\\Users\\kaanan\\Desktop\\Noses\\train_images.npy\", allow_pickle = True)\n",
    "train_bb_data = np.load(r\"C:\\Users\\kaanan\\Desktop\\Noses\\train_bb_data.npy\", allow_pickle = True)\n",
    "train_images, train_targets = get_data(train_img_data, train_bb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(preds, score_thresh, nms_thresh, targets, device = device):\n",
    "    preds = [decode_prediction(pred, score_thresh, nms_thresh) for pred in preds]\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(preds, targets)\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model \n",
    "rcnn_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\RCNN Models\\Additional Data\\rcnn_extra_data_30\"\n",
    "rcnn = get_object_detection_model(rcnn_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_less_data = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\RCNN Evaluation Information\\Trial 2\\Models\\rcnn_trial2_50\"\n",
    "rcnn_less = get_object_detection_model(rcnn_less_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rcnn_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\RCNN Models\\Additional Data\\rcnn_extra_data_v2_5\"\n",
    "new_rcnn = get_object_detection_model(new_rcnn_path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rcnn_more_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\RCNN Models\\Additional Data\\rcnn_extra_data_v2_more_5\"\n",
    "new_rcnn_more = get_object_detection_model(new_rcnn_path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005007743835449219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11199,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a6175d048d4763a20c92beacc62dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003961324691772461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11199,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffa511a50cc4aee845e8fb0f3223a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcnn_preds = get_seal_preds(train_images, rcnn, device)\n",
    "rcnn_less_preds = get_seal_preds(train_images, rcnn_less, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013031482696533203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11199,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72718c8420464d92ba357d288aaf7907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_rcnn_preds = get_seal_preds(train_images, new_rcnn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03719377517700195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11199,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5264170be1e54703a88fc5747dc3bc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_rcnn_more_preds = get_seal_preds(train_images, new_rcnn_more, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_iou(preds, targets):\n",
    "    avg_iou = 0\n",
    "    for i in range(len(preds)):\n",
    "        ious = ops.box_iou(targets[i][\"boxes\"], preds[i][\"boxes\"]).numpy()\n",
    "        if ious.shape[1] > 0:\n",
    "            avg_iou += ious.max(axis = 1).mean()\n",
    "    return avg_iou / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037890584486934215"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_iou(new_rcnn_preds, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01818738097770744"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_iou(rcnn_preds, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03553283928556558"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_iou(rcnn_less_preds, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007192998406463572"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_iou(rcnn_preds_cleaned, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(2.1645e-05),\n",
       " 'map_50': tensor(4.8298e-05),\n",
       " 'map_75': tensor(1.5671e-05),\n",
       " 'map_small': tensor(2.1645e-05),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(0.0004),\n",
       " 'mar_10': tensor(0.0008),\n",
       " 'mar_100': tensor(0.0008),\n",
       " 'mar_small': tensor(0.0019),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(rcnn_preds, 0, .2, train_targets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(2.2641e-05),\n",
       " 'map_50': tensor(5.8287e-05),\n",
       " 'map_75': tensor(1.0391e-05),\n",
       " 'map_small': tensor(2.2641e-05),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(0.0005),\n",
       " 'mar_10': tensor(0.0012),\n",
       " 'mar_100': tensor(0.0012),\n",
       " 'mar_small': tensor(0.0028),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(rcnn_less_preds, 0, .2, train_targets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(6.5107e-06),\n",
       " 'map_50': tensor(3.1633e-05),\n",
       " 'map_75': tensor(0.),\n",
       " 'map_small': tensor(6.5107e-06),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(1.6794e-05),\n",
       " 'mar_10': tensor(3.9185e-05),\n",
       " 'mar_100': tensor(3.9185e-05),\n",
       " 'mar_small': tensor(8.7353e-05),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(new_rcnn_preds, 0, .2, train_targets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(2.2446e-05),\n",
       " 'map_50': tensor(4.9314e-05),\n",
       " 'map_75': tensor(1.6540e-05),\n",
       " 'map_small': tensor(2.2446e-05),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(0.0004),\n",
       " 'mar_10': tensor(0.0006),\n",
       " 'mar_100': tensor(0.0006),\n",
       " 'mar_small': tensor(0.0014),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(rcnn_preds, 0, .05, train_targets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(2.2612e-05),\n",
       " 'map_50': tensor(5.6431e-05),\n",
       " 'map_75': tensor(1.0713e-05),\n",
       " 'map_small': tensor(2.2612e-05),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(0.0005),\n",
       " 'mar_10': tensor(0.0007),\n",
       " 'mar_100': tensor(0.0007),\n",
       " 'mar_small': tensor(0.0017),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(rcnn_less_preds, 0, .05, train_targets, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': tensor(6.7204e-06),\n",
       " 'map_50': tensor(3.2533e-05),\n",
       " 'map_75': tensor(0.),\n",
       " 'map_small': tensor(6.7204e-06),\n",
       " 'map_medium': tensor(0.),\n",
       " 'map_large': tensor(0.),\n",
       " 'mar_1': tensor(1.6794e-05),\n",
       " 'mar_10': tensor(3.9185e-05),\n",
       " 'mar_100': tensor(3.9185e-05),\n",
       " 'mar_small': tensor(8.7353e-05),\n",
       " 'mar_medium': tensor(0.),\n",
       " 'mar_large': tensor(0.),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_map(new_rcnn_preds, 0, .05, train_targets, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
