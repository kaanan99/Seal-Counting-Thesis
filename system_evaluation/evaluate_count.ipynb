{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "sys.path.append(\"../seal_counter/Notebooks/RCNN Notebooks\")\n",
    "from rcnn_utils import (\n",
    "    decode_prediction, \n",
    "    get_bb, \n",
    "    get_object_detection_model,\n",
    "    predict, \n",
    "    write_to_latex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GPU if one exists.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using: \", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN predictions\n",
    "training_pred_path = \"../seal_detector/Generated Data/training_seals_pytorch.pkl\"\n",
    "val_pred_path = \"../seal_detector/Generated Data/validation_seals_pytorch.pkl\"\n",
    "test_pred_path = \"../seal_detector/Generated Data/testing_seals_pytorch.pkl\"\n",
    "\n",
    "with open(training_pred_path, \"rb\") as f:\n",
    "    train_preds = pickle.load(f)\n",
    "with open(val_pred_path, \"rb\") as f:\n",
    "    val_preds = pickle.load(f)\n",
    "with open(test_pred_path, \"rb\") as f:\n",
    "    test_preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "rcnn_unfrozen = get_object_detection_model(path=\"../seal_counter/Models/rcnn_extra_data_base_30_10\", version=1)\n",
    "rcnn_frozen_v1 = get_object_detection_model(path=\"../seal_counter/Models/rcnn_trial1_50\", version=1)\n",
    "rcnn_frozen_v2 = get_object_detection_model(path=\"../seal_counter/Models/rcnn_trial3_50\", version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path to images\n",
    "training_image_path = \"../Training, Val, and Test Images/Training Images/\"\n",
    "validation_image_path = \"../Training, Val, and Test Images/Validation Images/\"\n",
    "testing_image_path = \"../Training, Val, and Test Images/Test Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models:List, model_names:List[str], preds_dict:Dict, nms_thresh:float, score_thresh:float, xml_path:str) -> pd.DataFrame:\n",
    "    \"\"\"Compares the count of the provided models for a specific image\n",
    "\n",
    "    Args:\n",
    "        models (List): List of pytorch models to compare\n",
    "        model_names (List[str]): Names of the models\n",
    "        preds_dict (Dict): Dictionary mapping image file name to RCNN predictions for that image\n",
    "        nms_thresh (float): NMS thresold\n",
    "        score_thresh (float): Score threshold\n",
    "        xml_path (str): path to image XML\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing actual and predicted seal counts for each model\n",
    "    \"\"\"\n",
    "    \n",
    "    file_names = [file_name for file_name in preds_dict.keys()]\n",
    "    counts = [[] for _ in models]\n",
    "    actual_count = []\n",
    "\n",
    "    # Generate counts for each image in dataset\n",
    "    for file_name in tqdm(file_names):\n",
    "\n",
    "        # Initialize count for each model to 0 and get actual count\n",
    "        image_count = [0 for _ in models]\n",
    "        actual_count.append(\n",
    "            get_bb(xml_path, [file_name+\".xml\"]).shape[0]\n",
    "            )\n",
    "        \n",
    "        # Iterate through  each sub-image per iamge\n",
    "        for sub_img in preds_dict[file_name]:\n",
    "            \n",
    "            # Get prediction for specific sub-image for each model\n",
    "            for i in range(len(image_count)):\n",
    "                pred = decode_prediction(predict(models[i], sub_img), score_thresh, nms_thresh)\n",
    "                image_count[i] += len(pred)\n",
    "        \n",
    "        # Update list of counts for image with model counts\n",
    "        for i in range(len(image_count)):\n",
    "            counts[i].append(image_count[i])\n",
    "\n",
    "    # DataFrame containing counts\n",
    "    df = pd.DataFrame({\"File Name\":file_names, \"Actual Count\": actual_count})\n",
    "\n",
    "    # Printing Metrics\n",
    "    for i in range(len(model_names)):\n",
    "        \n",
    "        # Constants\n",
    "        model_name = model_names[i]\n",
    "        df[model_name] = counts[i]\n",
    "        absolute_difference = abs(df[\"Actual Count\"] - df[model_name])\n",
    "\n",
    "        # Metric Calculation\n",
    "        mean_absolute_percent_error = (absolute_difference / df[\"Actual Count\"]).mean()\n",
    "        mean_absolute_error = absolute_difference.mean()\n",
    "        error_per_ten_seals = (mean_absolute_error * 10) / df[\"Actual Count\"].mean()\n",
    "        total_miscounted_seals = absolute_difference.sum()\n",
    "   \n",
    "        # Print Statements\n",
    "        print(f\"Metrics for model: {model_name}\")\n",
    "\n",
    "        name_to_metric_map = {\n",
    "            \"Mean Absolute Percent Error\": mean_absolute_percent_error,\n",
    "            \"Mean Absolute Error\": mean_absolute_error,\n",
    "            \"Error per 10 seals\": error_per_ten_seals,\n",
    "            \"Total Miscounted Seals\" : total_miscounted_seals\n",
    "        }\n",
    "        max_string_length = max(\n",
    "            [len(metric_name) for metric_name in name_to_metric_map.keys()]\n",
    "        )\n",
    "        for metric_name in name_to_metric_map.keys():\n",
    "            print(f\"\\t{metric_name:<{max_string_length + 1}}: {name_to_metric_map[metric_name]:>5.4f}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model lists\n",
    "unfrozen_model_list = [rcnn_unfrozen,]\n",
    "unfrozen_model_names = [\"Unfrozen V1\"]\n",
    "\n",
    "frozen_model_list = [ rcnn_frozen_v1, rcnn_frozen_v2]\n",
    "frozen_model_names = [ \"Frozen V1\", \"Frozen V2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_count(\n",
    "        unfrozen_model_list:List, \n",
    "        unfrozen_model_names:List[str], \n",
    "        frozen_model_list:List, \n",
    "        frozen_model_names:List[str], \n",
    "        preds:Dict, \n",
    "        image_path:str, \n",
    "        nms_thresh:float=.1, \n",
    "        unfrozen_score_thresh:float=.75, \n",
    "        frozen_thresh:float=.65\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"Creates Dataframe containing true and predicted counts of specified frozen and unfrozen models.\n",
    "       Unfrozen and frozen models can use different scores.\n",
    "\n",
    "    Args:\n",
    "        unfrozen_model_list (List): List of unfrozen pytorch models\n",
    "        unfrozen_model_names (List[str]): List of unfrozen model names\n",
    "        frozen_model_list (List): List of frozen pytorch models\n",
    "        frozen_model_names (List[str]): LList of frozen model names\n",
    "        preds (Dict): Dictionary mapping image name to RCNN predictions for dataset\n",
    "        image_path (str): Path to images/xml files in the dataset\n",
    "        nms_thresh (float, optional): NMS threshold. Defaults to .1.\n",
    "        unfrozen_score_thresh (float, optional): Score threshold to be used for unfrozen models. Defaults to .75.\n",
    "        frozen_thresh (float, optional): Score threshold to be used for frozen models. Defaults to .65.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing actual and predicted counts for all models specified\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get df for unfrozen models\n",
    "    unfrozen_results = compare_models(\n",
    "            unfrozen_model_list, \n",
    "            unfrozen_model_names, \n",
    "            preds, \n",
    "            nms_thresh, \n",
    "            unfrozen_score_thresh, \n",
    "            image_path,\n",
    "        )\n",
    "    \n",
    "    # Get df for frozen models \n",
    "    frozen_results = compare_models(\n",
    "            frozen_model_list, \n",
    "            frozen_model_names, \n",
    "            preds, \n",
    "            nms_thresh, \n",
    "            frozen_thresh,\n",
    "            image_path,\n",
    "        )\n",
    "    \n",
    "    # Combine dfs\n",
    "    merge_columns = [\"File Name\", \"Actual Count\"]\n",
    "    return unfrozen_results.merge(frozen_results, right_on=merge_columns, left_on=merge_columns).sort_values(by=\"File Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = evaluate_count(unfrozen_model_list, unfrozen_model_names, frozen_model_list, frozen_model_names, train_preds, training_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = evaluate_count(unfrozen_model_list, unfrozen_model_names, frozen_model_list, frozen_model_names, val_preds, validation_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_count(unfrozen_model_list, unfrozen_model_names, frozen_model_list, frozen_model_names, test_preds, testing_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results.to_csv(\"training_results.csv\", index=False)\n",
    "validation_results.to_csv(\"validation_results.csv\", index=False)\n",
    "test_results.to_csv(\"test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to latex (if you want to put the results in a paper)\n",
    "\n",
    "# write_to_latex(training_results, \"training_results\", long_table=True)\n",
    "# write_to_latex(validation_results, \"validation_results\", long_table=True)\n",
    "# write_to_latex(test_results, \"test_results\", long_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
