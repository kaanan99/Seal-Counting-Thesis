{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "sys.path.append(\"../RCNN Notebooks\")\n",
    "from rcnn_utils import decode_prediction, get_bb, parse_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df_directory = \"Grid Search DataFrames\"\n",
    "\n",
    "if grid_search_df_directory not in os.listdir():\n",
    "    os.mkdir(grid_search_df_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score and NMS Treshold Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_count(path:str, file_name:str) -> int:\n",
    "   \"\"\"Gets the actual count of seals in an image\n",
    "\n",
    "   Args:\n",
    "         path (str): Path to directory containing images/XML files\n",
    "         file_name (str): Name of XML file\n",
    "\n",
    "   Returns:\n",
    "         int: Total number of seals within the image\n",
    "   \"\"\"\n",
    "   xml_name = file_name + \".xml\"\n",
    "   return get_bb(path, [xml_name]).shape[0]\n",
    "\n",
    "\n",
    "def get_image_score_results(image_predictions:List, nms_threshold:float, actual_count:int) -> Tuple[float, int, int]:\n",
    "    \"\"\"Calculate the predicted seal count and the difference from the true count for all seal predictions from an image\n",
    "\n",
    "    Args:\n",
    "        image_predictions (List): List RCNN predictions made on sub-images for an image\n",
    "        nms_threshold (float): NMS threshold to use on bounding box predictions\n",
    "        actual_count (int): Actual number of seals within an image\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, int, int]: Returns (Score, Predicted number of seals in the image, Difference between predicted number and actual number of seals)\n",
    "    \"\"\"\n",
    "    score_values = []\n",
    "    count_predictions = []\n",
    "    count_differences = []\n",
    "\n",
    "    potential_scores = [round(x, 2) for x in np.arange(0.0, 1.0, 0.05)]\n",
    "    for score in potential_scores:\n",
    "        predicted_count = 0\n",
    "        for prediction in image_predictions:\n",
    "            boxes, scores, labels = decode_prediction(prediction, score, nms_threshold)\n",
    "            predicted_count += len(boxes)\n",
    "\n",
    "        # Update Values\n",
    "        score_values.append(score)\n",
    "        count_predictions.append(predicted_count)\n",
    "        count_differences.append(abs(actual_count - predicted_count))\n",
    "        \n",
    "    return score_values, count_predictions, count_differences\n",
    "\n",
    "\n",
    "def conduct_grid_search(dataset_predictions:List, dataset_image_path:str, write_path:str=None, name:str=\"\") -> pd.DataFrame:\n",
    "    \"\"\"Conduct a grid search where multiple score and nms thresholds are tested for all images in the dataset\n",
    "\n",
    "    Args:\n",
    "        dataset_predictions (List): List of RCNN bounding box predictions for that dataset\n",
    "        dataset_image_path (str): path to directory containing Images/XML files for the dataset\n",
    "        write_path (str, optional): Path to the location where the dataframe should be saved. Defaults to None.\n",
    "        name (str, optional): Name that the dataframe will be saved with. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame containing grid search information\n",
    "    \"\"\"\n",
    "    # Initialize dict to store df information\n",
    "    data_frame_dict = {\n",
    "        \"File Name\": [], \n",
    "        \"Score\": [], \n",
    "        \"IOU Threshold\": [], \n",
    "        \"Predicted Counts\": [], \n",
    "        \"Actual Count\": [], \n",
    "        \"Count Difference\": []\n",
    "    }\n",
    "\n",
    "    nms_thresholds = [round(x, 2) for x in np.arange(0.0, 1.0, 0.05)]\n",
    "\n",
    "    for image_name in tqdm(dataset_predictions.keys()):\n",
    "        # Initialize image variables\n",
    "        actual_image_counts = get_actual_count(dataset_image_path, image_name)\n",
    "        image_predictions = dataset_predictions[image_name]\n",
    "\n",
    "        for nms_thresh in nms_thresholds:\n",
    "\n",
    "            # Calculate predicted count of all score for an image\n",
    "            score_values, count_predictions, count_differences = get_image_score_results(image_predictions, nms_thresh, actual_image_counts)\n",
    "            num_observations = len(score_values)\n",
    "            \n",
    "            # Update values in dataframe\n",
    "            data_frame_dict[\"File Name\"] += [image_name] * num_observations\n",
    "            data_frame_dict[\"Score\"] += score_values\n",
    "            data_frame_dict[\"IOU Threshold\"] += [nms_thresh] * num_observations\n",
    "            data_frame_dict[\"Predicted Counts\"] += count_predictions\n",
    "            data_frame_dict[\"Actual Count\"] += [actual_image_counts] * num_observations\n",
    "            data_frame_dict[\"Count Difference\"] += count_differences\n",
    "    \n",
    "    grid_search_df = pd.DataFrame(data_frame_dict)\n",
    "\n",
    "    # Save CSV\n",
    "    if write_path is not None:\n",
    "        if name != \"\":\n",
    "            name = \"_\"+name\n",
    "        grid_search_df.to_csv(\"{}/grid_search{}.csv\".format(write_path, name))\n",
    "    \n",
    "    return grid_search_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"unfrozen\", \"frozen_v1\", \"frozen_v2\"]\n",
    "\n",
    "dataset_image_paths = {\n",
    "    \"training\": \"Training Images\",\n",
    "    \"validation\": \"Validation Images\",\n",
    "    \"testing\": \"Test Images\"\n",
    "}\n",
    "\n",
    "dataset_types = list(dataset_image_paths.keys())\n",
    "write_path = grid_search_df_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    \n",
    "    print(\"Generating Predictions for:\", model_name)\n",
    "    \n",
    "    for dataset_type in dataset_types:\n",
    "        \n",
    "        print(\"\\tUsing dataset:\", dataset_type)\n",
    "        \n",
    "        write_name = f\"{model_name}_{dataset_type}\"\n",
    "\n",
    "        dataset_image_path = f\"../../../Training, Val, and Test Images/{dataset_image_paths[dataset_type]}/\"\n",
    "        \n",
    "        dataset_path = f\"../../Generated Data/{model_name}_{dataset_type}_predictions.pkl\"\n",
    "        with open(dataset_path, \"rb\") as fp:\n",
    "            rcnn_predictions = pickle.load(fp)\n",
    "        \n",
    "        grid_search_df = conduct_grid_search(rcnn_predictions, dataset_image_path, write_path, write_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centriod_metrics(image_centriods:List[Tuple[float, float]], eps:float) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Calculates all metrics about centriods from a list of centriods\n",
    "       Calculated Metrics:\n",
    "       - Number of sub-images in image that contain a seal\n",
    "       - Number of clusters\n",
    "       - Number of sub-images in the largest cluster\n",
    "       - Number of sub-images in the smallest cluster\n",
    "\n",
    "    Args:\n",
    "        image_centriods (List[Tuple[float, float]]): (x, y) coordinates of centriods\n",
    "        eps (float): Epsilon value to be used in DBSCAN\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, int, int]: List of relevent metrics\n",
    "    \"\"\"\n",
    "    if len(image_centriods) > 0:\n",
    "        # Calculate clusters\n",
    "        clustering_object = DBSCAN(eps=eps, min_samples=1).fit(image_centriods)\n",
    "        labels = pd.Series(clustering_object.labels_)\n",
    "\n",
    "        # Filter out invalid clusters\n",
    "        valid_cluster_indices = labels > -1\n",
    "        labels = labels[valid_cluster_indices].value_counts()\n",
    "\n",
    "        # Calculate metrics\n",
    "        seal_sub_image_number = labels.sum()\n",
    "        cluster_number = len(labels)\n",
    "        largest_cluster = labels.max()\n",
    "        smallest_cluster = labels.min()\n",
    "\n",
    "        return seal_sub_image_number, cluster_number, largest_cluster, smallest_cluster\n",
    "   \n",
    "   # If there are no seals in the image\n",
    "    else:\n",
    "        return 0, 0, 0, 0\n",
    "    \n",
    "\n",
    "def conduct_centriod_grid_search(centriods:Dict, epsilon_values:List[int], write_path:str=None, write_name:str=None) -> pd.DataFrame:\n",
    "    \"\"\"Calculates cluster metrics for each specified epsilon value for all images in the dataset\n",
    "\n",
    "    Args:\n",
    "        centriods (Dict[str:List]): Dictionary mapping image name to list of centriods for a dataset\n",
    "        epsilon_values (List[int], optional): An array of epsilon values to be used with DBSCAN.\n",
    "        write_path (str, optional): Path to location to save dataframe. Defaults to None.\n",
    "        write_name (str, optional): Name of saved file. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all cluster grid search information\n",
    "    \"\"\"\n",
    "    # Dictionary containing all data frame information\n",
    "    data_frame_dict = {\n",
    "        \"Image Name\": [],\n",
    "        \"Epsilon Value\": [],\n",
    "        \"Sub-Images with Seals\": [],\n",
    "        \"Number of Clusters\": [],\n",
    "        \"Largest Cluster Size\": [],\n",
    "        \"Smallest Cluster Size\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "    # Iterate through each image\n",
    "    for image_name in tqdm(centriods.keys()):\n",
    "        image_centriods = centriods[image_name]\n",
    "        \n",
    "        # Iterate through each espilon value\n",
    "        for epsilon_value in epsilon_values:\n",
    "            \n",
    "            # Calculate cluster metrics\n",
    "            seal_sub_image_number, cluster_number, largest_cluster, smallest_cluster = calculate_centriod_metrics(image_centriods, epsilon_value)\n",
    "\n",
    "            # Save metrics in dictionary\n",
    "            data_frame_dict[\"Image Name\"].append(image_name)\n",
    "            data_frame_dict[\"Epsilon Value\"].append(epsilon_value)\n",
    "            data_frame_dict[ \"Sub-Images with Seals\"].append(seal_sub_image_number)\n",
    "            data_frame_dict[\"Number of Clusters\"].append(cluster_number)\n",
    "            data_frame_dict[\"Largest Cluster Size\"].append(largest_cluster)\n",
    "            data_frame_dict[\"Smallest Cluster Size\"].append(smallest_cluster)\\\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    centriod_info_df = pd.DataFrame(data_frame_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    if write_path is not None:\n",
    "        if write_name is not None:\n",
    "            centriod_info_df.to_csv(f\"{write_path}/centriod_info_{write_name}.csv\", index=False)\n",
    "        else:\n",
    "            centriod_info_df.to_csv(f\"{write_path}/centriod_info.csv\", index=False)\n",
    "\n",
    "    return centriod_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon Values to conduct grid search over\n",
    "epsilon_values = [150, 300, 450]\n",
    "\n",
    "# Iterate through models\n",
    "for model_name in models:\n",
    "    print(\"Generating Grid Search for model:\", model_name)\n",
    "    \n",
    "    # Iterate through datasets\n",
    "    for dataset_type in dataset_types:\n",
    "        print(\"\\tUsing Dataset:\", dataset_type)\n",
    "\n",
    "        write_name = f\"{model_name}_{dataset_type}\"\n",
    "\n",
    "        # Load centriods for dataset\n",
    "        centriods_path = f\"Centroids/seals_centroids_{dataset_type}.pkl\"\n",
    "        with open(centriods_path, \"rb\") as fp:\n",
    "                    centriods = pickle.load(fp)\n",
    "        \n",
    "        # Conduct Grid Search\n",
    "        conduct_centriod_grid_search(\n",
    "               centriods,\n",
    "               epsilon_values,\n",
    "               write_path, \n",
    "               write_name\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
