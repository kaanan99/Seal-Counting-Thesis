{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from torchvision import ops\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\\RCNN Notebooks\")\n",
    "from rcnn_utils import decode_prediction, write_to_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Score predictor\n",
    "model_name = \"unfrozen\"\n",
    "epsilon_value = 300\n",
    "nms_threshold = 0.2\n",
    "data_reduction_type = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RCNN predictions\n",
    "\n",
    "training_prediction_path = f\"../../Generated Data/{model_name}_training_predictions.pkl\"\n",
    "validation_prediction_path = f\"../../Generated Data/{model_name}_validation_predictions.pkl\"\n",
    "testing_prediction_path = f\"../../Generated Data/{model_name}_testing_predictions.pkl\"\n",
    "\n",
    "\n",
    "with open(training_prediction_path, \"rb\") as fp:\n",
    "    training_predictions = pickle.load(fp)\n",
    "with open(validation_prediction_path, \"rb\") as fp:\n",
    "    validation_predictions = pickle.load(fp)\n",
    "with open(testing_prediction_path, \"rb\") as fp:\n",
    "    testing_predictions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Score Predictor DataFrames\n",
    "\n",
    "training_df = pd.read_csv(f\"Score Predictor DataFrames/{model_name}_training_ep_{epsilon_value}_nms_{nms_threshold}_{data_reduction_type}.csv\")\n",
    "validation_df = pd.read_csv(f\"Score Predictor DataFrames/{model_name}_validation_ep_{epsilon_value}_nms_{nms_threshold}_{data_reduction_type}.csv\")\n",
    "testing_df = pd.read_csv(f\"Score Predictor DataFrames/{model_name}_testing_ep_{epsilon_value}_nms_{nms_threshold}_{data_reduction_type}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Score predictors\n",
    "\n",
    "input_columns = ['Sub-Images with Seals', 'Number of Clusters','Largest Cluster Size', 'Smallest Cluster Size']\n",
    "\n",
    "y_train = training_df[\"Score\"]\n",
    "x_train = training_df[input_columns]\n",
    "\n",
    "# Various Score predictor models\n",
    "knn = KNeighborsRegressor()\n",
    "linear_regression = LinearRegression()\n",
    "random_forest = RandomForestRegressor(random_state=0)\n",
    "\n",
    "model_names = [\"KNN Regressor\", \"Linear Regression\", \"Random Forest Regression\"]\n",
    "models = [knn, linear_regression, random_forest]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score_predictor_counts(df:pd.DataFrame, predictions:Dict, model) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluates the performance of a score predictor model by calculating several metrics\n",
    "       Calculated Metrics:\n",
    "       - Mean Absolute Percent Error\n",
    "       - Mean Absolute Error\n",
    "       - Error per 10 seals\n",
    "       - Total miscounted Seals\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Score predictor dataframe for a given dataset \n",
    "        predictions (Dict): RCNN predictions for a specified dataset\n",
    "        model (SciKit Learn Model): Score predictor model being evaluated\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: Tuple containing all metrics\n",
    "    \"\"\"\n",
    "    # Predict Scores\n",
    "    evaluation_df = df[[\"File Name\", \"Actual Count\"]]\n",
    "    evaluation_df[\"Predicted Score\"] = model.predict(df[input_columns])\n",
    "\n",
    "    predicted_counts = []\n",
    "\n",
    "    # Get Predicted Count for Each Image\n",
    "    for idx in range(evaluation_df.shape[0]):\n",
    "        row = evaluation_df.iloc[idx]\n",
    "\n",
    "        image_name = row[\"File Name\"]\n",
    "        predicted_score = row[\"Predicted Score\"]\n",
    "\n",
    "        # Get Predicted Count for one image\n",
    "        image_count = 0\n",
    "        image_predictions = predictions[image_name]\n",
    "\n",
    "        for sub_image_predicition in image_predictions:\n",
    "            boxes, scores, labels = decode_prediction(sub_image_predicition, predicted_score, nms_threshold, use_numpy=True)\n",
    "            image_count += len(boxes)\n",
    "\n",
    "        predicted_counts.append(image_count)\n",
    "\n",
    "    evaluation_df[\"Predicted Count\"] = predicted_counts\n",
    "    absolute_difference = abs(evaluation_df[\"Actual Count\"] - evaluation_df[\"Predicted Count\"])\n",
    "\n",
    "    # Metric Calculation\n",
    "    mean_absolute_percent_error = (absolute_difference / evaluation_df[\"Actual Count\"]).mean()\n",
    "    mean_absolute_error = absolute_difference.mean()\n",
    "    error_per_ten_seals = (mean_absolute_error * 10) / evaluation_df[\"Actual Count\"].mean()\n",
    "    total_miscounted_seals = absolute_difference.sum()\n",
    "\n",
    "    return mean_absolute_percent_error, mean_absolute_error, error_per_ten_seals, total_miscounted_seals\n",
    "\n",
    "\n",
    "def compare_score_predictors(models:List, model_names:List[str], df:pd.DataFrame, predictions:Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generates a dataframe comparing score predictor performance\n",
    "\n",
    "    Args:\n",
    "        models (List): List of Scikit Learn models\n",
    "        model_names (List[str]): Names of the scikit learn models\n",
    "        df (pd.DataFrame): Score predictor dataframe for specific dataset\n",
    "        predictions (Dict): RCNN predictions for specific dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    data_frame_dict = {\n",
    "        \"Model Name\": [],\n",
    "        \"Mean Absolute Percent Error\": [],\n",
    "        \"Mean Absolute Error\": [],\n",
    "        \"Error per 10 Seals\": [],\n",
    "        \"Total Miscounted Seals\": [],\n",
    "    }\n",
    "\n",
    "    # Iterate through each score predictor model\n",
    "    for idx in range(len(models)):\n",
    "        model = models[idx]\n",
    "        model_name = model_names[idx]\n",
    "\n",
    "        # Calculate metric\n",
    "        mean_absolute_percent_error, mean_absolute_error, error_per_ten_seals, total_miscounted_seals = evaluate_score_predictor_counts(df, predictions, model)\n",
    "        \n",
    "        # Save Metrics\n",
    "        data_frame_dict[\"Model Name\"].append(model_name)\n",
    "        data_frame_dict[\"Mean Absolute Percent Error\"].append(mean_absolute_percent_error)\n",
    "        data_frame_dict[\"Mean Absolute Error\"].append(mean_absolute_error)\n",
    "        data_frame_dict[\"Error per 10 Seals\"].append(error_per_ten_seals)\n",
    "        data_frame_dict[\"Total Miscounted Seals\"].append(total_miscounted_seals)\n",
    "\n",
    "    return pd.DataFrame(data_frame_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = compare_score_predictors(models, model_names, training_df, training_predictions)\n",
    "training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = compare_score_predictors(models, model_names, validation_df, validation_predictions)\n",
    "validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_results = compare_score_predictors(models, model_names, testing_df, testing_predictions)\n",
    "testing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tables to latex (Incase you want to put table results in a paper)\n",
    "\n",
    "# write_to_latex(training_results, f\"{model_name}_score_predictor_training_ep_{epsilon_value}_nms_{nms_threshold}\")\n",
    "# write_to_latex(validation_results, f\"{model_name}_score_predictor_validation_ep_{epsilon_value}_nms_{nms_threshold}\")\n",
    "# write_to_latex(testing_results, f\"{model_name}_score_predictor_testing_ep_{epsilon_value}_nms_{nms_threshold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
