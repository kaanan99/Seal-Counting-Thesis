{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from torchvision import ops\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(prediction, \n",
    "                      score_threshold = 0.8, \n",
    "                      nms_iou_threshold = 0.2):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        prediction: dict\n",
    "        score_threshold: float\n",
    "        nms_iou_threshold: float\n",
    "    Returns\n",
    "        prediction: tuple\n",
    "    \"\"\"\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    scores = prediction[\"scores\"]\n",
    "    labels = prediction[\"labels\"]    \n",
    "    # Remove any low-score predictions.\n",
    "    if score_threshold is not None:\n",
    "        want = scores > score_threshold\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]    \n",
    "    # Remove any overlapping bounding boxes using NMS.\n",
    "    if nms_iou_threshold is not None:\n",
    "        want = ops.nms(boxes = boxes, scores = scores, iou_threshold = nms_iou_threshold)\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]    \n",
    "        return (boxes.numpy(), labels.numpy(), scores.numpy())\n",
    "    \n",
    "def get_best_counts(df, preds, thresh):\n",
    "    best_possible_count = []\n",
    "    for name in df[\"File Name\"]:\n",
    "        opt_score = df[df[\"File Name\"] == name][\"Score\"].values[0]\n",
    "        b_count = 0\n",
    "        for pred in preds[name]:\n",
    "                    boxes, scores, labels = decode_prediction(pred, opt_score, thresh)\n",
    "                    b_count += len(boxes)\n",
    "        best_possible_count.append(b_count)\n",
    "    return best_possible_count\n",
    "\n",
    "def get_counts(df, preds, pred_scores, thresh):\n",
    "    count = []\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :]\n",
    "        file_name = row[\"File Name\"]\n",
    "        image_count = 0\n",
    "        score = pred_scores[i]\n",
    "        for pred in preds[file_name]:\n",
    "              boxes, scores, labels = decode_prediction(pred, score, thresh)\n",
    "              image_count += len(boxes)\n",
    "        count.append(image_count)\n",
    "    return count\n",
    "\n",
    "def get_metrics(actual_count, pred_count):\n",
    "    ac = np.array(actual_count)\n",
    "    pc = np.array(pred_count)\n",
    "    average_error = abs(ac - pc).mean()\n",
    "    average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
    "    median_error = np.median(abs(ac-pc))\n",
    "    median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
    "    return average_error, average_percent_error, median_error, median_percent_error\n",
    "\n",
    "def eval_predictor(df, pred_info, pred_score, thresh, actual):\n",
    "    pred_counts = get_counts(df, pred_info, pred_score, thresh)\n",
    "    if actual:\n",
    "         return get_metrics(df[\"Actual Count\"], pred_counts)\n",
    "    else:\n",
    "        return get_metrics(df[\"Best Possible Counts\"], pred_counts)\n",
    "\n",
    "def get_total_boxes(df, preds, thresh):\n",
    "    total_box_num = []\n",
    "    for name in df[\"File Name\"]:\n",
    "        total_box_count = 0\n",
    "        for pred in preds[name]:\n",
    "            boxes, scores, labels = decode_prediction(pred, 0, thresh)\n",
    "            total_box_count += len(boxes)\n",
    "        total_box_num.append(total_box_count)\n",
    "    return total_box_num  \n",
    "\n",
    "def sci_get_metrics(model, x_train, y_train, x_val, df_training, df_validation, training_preds, val_preds, thresh, actual, baseline):\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_train) + baseline\n",
    "    train_ae, train_ape, train_me, train_mpe = eval_predictor(df_training, training_preds, preds, thresh, actual)\n",
    "    preds = model.predict(x_val) + baseline\n",
    "    val_ae, val_ape, val_me, val_mpe = eval_predictor(df_validation, val_preds, preds, thresh, actual)\n",
    "    return train_ae, train_ape, val_ae, val_ape, train_me, train_mpe, val_me, val_mpe\n",
    "\n",
    "def get_default_metrics(val, df_training, training_preds, df_val, val_preds, y_train, thresh, actual):\n",
    "     train_ae, train_ape, train_me, train_mpe = eval_predictor(df_training, training_preds, [val] * len(y_train), thresh, actual)\n",
    "     val_ae, val_ape, val_me, val_mpe = eval_predictor(df_val, val_preds, [val] * len(y_train), thresh, actual)\n",
    "     return train_ae, train_ape, val_ae, val_ape, train_me, train_mpe, val_me, val_mpe\n",
    "\n",
    "def generate_comparison(models, model_names, df_training, df_validation, training_preds, val_preds, thresh, default_scores, actual, baseline = 0):\n",
    "    training_aes = []\n",
    "    training_apes = []\n",
    "    val_aes = []\n",
    "    val_apes = []\n",
    "    training_mes = []\n",
    "    training_mpes = []\n",
    "    val_mes = []\n",
    "    val_mpes = []\n",
    "    y_train = df_training[\"Score\"] - baseline\n",
    "    x_train = df_training.drop(columns = [\"Score\", \"File Name\", \"Best Possible Counts\", \"Unnamed: 0\", \"Actual Count\"])\n",
    "    x_val = df_validation.drop(columns = [\"Score\", \"File Name\", \"Best Possible Counts\", \"Unnamed: 0\", \"Actual Count\"])\n",
    "    for model in models:\n",
    "        train_ae, train_ape, val_ae, val_ape, train_me, train_mpe, val_me, val_mpe = sci_get_metrics(model, x_train, y_train, x_val, df_training, df_validation, training_preds, val_preds, thresh, actual, baseline)\n",
    "        training_aes.append(train_ae)\n",
    "        training_apes.append(train_ape)\n",
    "        val_aes.append(val_ae)\n",
    "        val_apes.append(val_ape)\n",
    "        training_mes.append(train_me)\n",
    "        training_mpes.append(train_mpe)\n",
    "        val_mes.append(val_me)\n",
    "        val_mpes.append(val_mpe)\n",
    "    for val in default_scores:\n",
    "        train_ae, train_ape, val_ae, val_ape, train_me, train_mpe, val_me, val_mpe = get_default_metrics(val, df_training, training_preds, df_validation, val_preds, y_train, thresh, actual)\n",
    "        training_aes.append(train_ae)\n",
    "        training_apes.append(train_ape)\n",
    "        val_aes.append(val_ae)\n",
    "        val_apes.append(val_ape)\n",
    "        training_mes.append(train_me)\n",
    "        training_mpes.append(train_mpe)\n",
    "        val_mes.append(val_me)\n",
    "        val_mpes.append(val_mpe)\n",
    "    \n",
    "    d1 = {\"Model Name\": model_names, \"Training Average Error\": training_aes, \"Training Average Percent Error\":training_apes, \"Validation Average Error\":val_aes, \"Validation Average Percent Error\":val_apes}\n",
    "    d2 =  {\"Model Name\": model_names, \"Training Median Error\": training_mes, \"Training Median Percent Error\":training_mpes, \"Validation Median Error\":val_mes, \"Validation Median Percent Error\":val_mpes}\n",
    "    return pd.DataFrame(d1).set_index(\"Model Name\"), pd.DataFrame(d2).set_index(\"Model Name\")\n",
    "\n",
    "def get_least_score(grid_df):\n",
    "    df = grid_df.groupby(\"Score\")[\"Count Difference\"].sum().reset_index()\n",
    "    return df[df[\"Count Difference\"] == df[\"Count Difference\"].min()][\"Score\"].values[0]\n",
    "\n",
    "def distribution_analysis(grid_df):\n",
    "    df = grid_df.groupby([\"File Name\", \"Score\"]).min()\n",
    "    min_scores = df[df[\"Count Difference\"] == df[\"Count Difference\"].min()].reset_index()[[\"File Name\", \"Score\"]]\n",
    "    score_counts = min_scores.groupby(\"Score\").count().reset_index()\n",
    "    scores = []\n",
    "    for i in range(score_counts.shape[0]):\n",
    "        row = score_counts.iloc[i, :]\n",
    "        for x in range(int(row[\"File Name\"])):\n",
    "            scores.append(row[\"Score\"])\n",
    "    scores = np.array(scores)\n",
    "    bins = np.arange(0, 1, .05)\n",
    "    plt.hist(scores, bins)\n",
    "    plt.xticks(np.arange(0, 1, .1))\n",
    "    plt.xlabel(\"Optimum Score\")\n",
    "    plt.ylabel(\"Occurrence\")\n",
    "    plt.show()\n",
    "    print(\"Mean: {}\".format(scores.mean()))\n",
    "    print(\"Median: {}\".format(np.median(scores)))\n",
    "    print(\"Standard Deviation: {}\".format(np.std(scores)))\n",
    "\n",
    "def error_distribution(model, model_name, df, preds, thresh, df_val = None, percent = True, actual = False):\n",
    "    if actual:\n",
    "        if df_val is None:\n",
    "            ac = df[\"Actual Count\"]\n",
    "        else:\n",
    "            ac = df_val[\"Actual Count\"]\n",
    "    else:\n",
    "        if df_val is None:\n",
    "            ac = df[\"Best Possible Counts\"]\n",
    "        else:\n",
    "            ac = df_val[\"Best Possible Counts\"]\n",
    "    y = df[\"Score\"]\n",
    "    x = df.drop(columns = [\"Score\", \"File Name\", \"Best Possible Counts\", \"Unnamed: 0\", \"Actual Count\"])\n",
    "    model.fit(x, y)\n",
    "    if df_val is None:\n",
    "        pred_scores = model.predict(x)\n",
    "        pred_counts = get_counts(df, preds, pred_scores, thresh)\n",
    "\n",
    "    else:\n",
    "        x_val = df_val.drop(columns = [\"Score\", \"File Name\", \"Best Possible Counts\", \"Unnamed: 0\", \"Actual Count\"])\n",
    "        pred_scores = model.predict(x_val)\n",
    "        pred_counts = get_counts(df_val, preds, pred_scores, thresh)\n",
    "    pc = np.array(pred_counts)\n",
    "    if percent:\n",
    "        error = np.nan_to_num(abs(ac - pc)/ac, nan = pc)\n",
    "    else:\n",
    "        error = abs(ac - pc)\n",
    "    plt.hist(error)\n",
    "    if percent:\n",
    "        plt.xlabel(\"Absolute Count Percent Error\")\n",
    "    else:\n",
    "        plt.xlabel(\"Absolute Count Error\")\n",
    "    plt.title(\"Error Occurence for {}\".format(model_name))\n",
    "    plt.ylabel(\"Occurrence\")\n",
    "    plt.show()\n",
    "    print(\"Mean Error: {}\".format(error.mean()))\n",
    "    print(\"Median Error: {}\".format(np.median(error)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Values\n",
    "\n",
    "# Thesh of .2\n",
    "\n",
    "\n",
    "# EPS is 300 instead of 150 and includes all data\n",
    "\n",
    "# df_training = pd.read_csv(\"../Data/training_total_0.2_300_V2\")\n",
    "# df_validation = pd.read_csv(\"../Data/validation_total_0.2_300_V2\")\n",
    "# thresh = .2\n",
    "\n",
    "\n",
    "# thresh of .05\n",
    "\n",
    "#EPS is 300 and includes all data\n",
    "\n",
    "# df_training = pd.read_csv(\"../Data/training_total_0.05_300_V2\")\n",
    "# df_validation = pd.read_csv(\"../Data/validation_total_0.05_300_V2\")\n",
    "# thresh = .05\n",
    "\n",
    "\n",
    "df_training = pd.read_csv(\"../Data/training_total_0.2_300_new_model_40\")\n",
    "df_validation = pd.read_csv(\"../Data/validation_mean_0.2_300_new_model_40\")\n",
    "thresh = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training[\"Total Boxes\"] = get_total_boxes(df_training, training_preds, thresh)\n",
    "# df_validation[\"Total Boxes\"] = get_total_boxes(df_validation, val_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pred_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\MetaData\\training_preds_new_model_30\"\n",
    "val_pred_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\MetaData\\validation_preds_new_model_30\"\n",
    "grid_search_training_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\Data\\grid_seach_training_total_new_model_30.csv\"\n",
    "grid_search_val_path = r\"C:\\Users\\kaanan\\Desktop\\RCNN\\Data\\grid_seach_validation_total_new_model_30.csv\"\n",
    "\n",
    "with open(training_pred_path, \"rb\") as fp:\n",
    "    training_preds = pickle.load(fp)\n",
    "\n",
    "with open(val_pred_path, \"rb\") as fp:\n",
    "    val_preds = pickle.load(fp)\n",
    "\n",
    "grid_train = pd.read_csv(grid_search_training_path)\n",
    "grid_val = pd.read_csv(grid_search_val_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to Best Possible Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No RGB values\n",
    "# df_training = df_training[[\"File Name\", \"Box Num\", \"Score\", \"Actual Count\", \"Cluster Num\", \"Biggest Cluster\", \"Smallest Cluster\", 'Unnamed: 0']]\n",
    "# df_validation = df_validation[[\"File Name\", \"Box Num\", \"Score\", \"Actual Count\", \"Cluster Num\", \"Biggest Cluster\", \"Smallest Cluster\", 'Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[\"Best Possible Counts\"] = get_best_counts(df_training, training_preds, thresh)\n",
    "df_validation[\"Best Possible Counts\"] = get_best_counts(df_validation, val_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Average Error</th>\n",
       "      <th>Training Average Percent Error</th>\n",
       "      <th>Validation Average Error</th>\n",
       "      <th>Validation Average Percent Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN Regressor</th>\n",
       "      <td>5.149457</td>\n",
       "      <td>0.148808</td>\n",
       "      <td>27.6875</td>\n",
       "      <td>0.372660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>6.258152</td>\n",
       "      <td>0.201467</td>\n",
       "      <td>54.1250</td>\n",
       "      <td>0.452074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regression</th>\n",
       "      <td>3.904891</td>\n",
       "      <td>0.185252</td>\n",
       "      <td>25.2500</td>\n",
       "      <td>0.285740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Score for Training (0.95)</th>\n",
       "      <td>8.173913</td>\n",
       "      <td>0.263313</td>\n",
       "      <td>43.8125</td>\n",
       "      <td>0.233789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Score for Validation (0.9)</th>\n",
       "      <td>7.505435</td>\n",
       "      <td>0.226311</td>\n",
       "      <td>30.3125</td>\n",
       "      <td>0.182247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Training Average Error  \\\n",
       "Model Name                                                \n",
       "KNN Regressor                                  5.149457   \n",
       "Linear Regression                              6.258152   \n",
       "Random Forest Regression                       3.904891   \n",
       "Best Score for Training (0.95)                 8.173913   \n",
       "Best Score for Validation (0.9)                7.505435   \n",
       "\n",
       "                                 Training Average Percent Error  \\\n",
       "Model Name                                                        \n",
       "KNN Regressor                                          0.148808   \n",
       "Linear Regression                                      0.201467   \n",
       "Random Forest Regression                               0.185252   \n",
       "Best Score for Training (0.95)                         0.263313   \n",
       "Best Score for Validation (0.9)                        0.226311   \n",
       "\n",
       "                                 Validation Average Error  \\\n",
       "Model Name                                                  \n",
       "KNN Regressor                                     27.6875   \n",
       "Linear Regression                                 54.1250   \n",
       "Random Forest Regression                          25.2500   \n",
       "Best Score for Training (0.95)                    43.8125   \n",
       "Best Score for Validation (0.9)                   30.3125   \n",
       "\n",
       "                                 Validation Average Percent Error  \n",
       "Model Name                                                         \n",
       "KNN Regressor                                            0.372660  \n",
       "Linear Regression                                        0.452074  \n",
       "Random Forest Regression                                 0.285740  \n",
       "Best Score for Training (0.95)                           0.233789  \n",
       "Best Score for Validation (0.9)                          0.182247  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_train_score = get_least_score(grid_train)\n",
    "best_val_score = get_least_score(grid_val)\n",
    "model_names = [\"KNN Regressor\", \"Linear Regression\", \"Random Forest Regression\", \"Best Score for Training ({})\".format(best_train_score), \"Best Score for Validation ({})\".format(best_val_score)]\n",
    "models = [KNeighborsRegressor(), LinearRegression(), RandomForestRegressor(random_state=0)]\n",
    "df_mean, df_median = generate_comparison(models, model_names, df_training, df_validation, training_preds, val_preds, thresh, [best_train_score, best_val_score], False)\n",
    "df_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to Actual Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  average_percent_error = np.nan_to_num(abs(ac - pc)/ac, nan = pc).mean()\n",
      "C:\\Users\\kaanan\\AppData\\Local\\Temp\\ipykernel_4348\\3316255726.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  median_percent_error = np.median(np.nan_to_num(abs(ac - pc)/ac, nan = pc))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Average Error</th>\n",
       "      <th>Training Average Percent Error</th>\n",
       "      <th>Validation Average Error</th>\n",
       "      <th>Validation Average Percent Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN Regressor</th>\n",
       "      <td>8.578804</td>\n",
       "      <td>0.426973</td>\n",
       "      <td>40.3750</td>\n",
       "      <td>0.600887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>7.991848</td>\n",
       "      <td>0.365408</td>\n",
       "      <td>44.8125</td>\n",
       "      <td>0.546558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regression</th>\n",
       "      <td>6.910326</td>\n",
       "      <td>0.359556</td>\n",
       "      <td>37.0625</td>\n",
       "      <td>0.486748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Score for Training (0.95)</th>\n",
       "      <td>7.771739</td>\n",
       "      <td>0.231875</td>\n",
       "      <td>25.7500</td>\n",
       "      <td>0.265007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Score for Validation (0.9)</th>\n",
       "      <td>7.864130</td>\n",
       "      <td>0.299939</td>\n",
       "      <td>19.2500</td>\n",
       "      <td>0.268273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Training Average Error  \\\n",
       "Model Name                                                \n",
       "KNN Regressor                                  8.578804   \n",
       "Linear Regression                              7.991848   \n",
       "Random Forest Regression                       6.910326   \n",
       "Best Score for Training (0.95)                 7.771739   \n",
       "Best Score for Validation (0.9)                7.864130   \n",
       "\n",
       "                                 Training Average Percent Error  \\\n",
       "Model Name                                                        \n",
       "KNN Regressor                                          0.426973   \n",
       "Linear Regression                                      0.365408   \n",
       "Random Forest Regression                               0.359556   \n",
       "Best Score for Training (0.95)                         0.231875   \n",
       "Best Score for Validation (0.9)                        0.299939   \n",
       "\n",
       "                                 Validation Average Error  \\\n",
       "Model Name                                                  \n",
       "KNN Regressor                                     40.3750   \n",
       "Linear Regression                                 44.8125   \n",
       "Random Forest Regression                          37.0625   \n",
       "Best Score for Training (0.95)                    25.7500   \n",
       "Best Score for Validation (0.9)                   19.2500   \n",
       "\n",
       "                                 Validation Average Percent Error  \n",
       "Model Name                                                         \n",
       "KNN Regressor                                            0.600887  \n",
       "Linear Regression                                        0.546558  \n",
       "Random Forest Regression                                 0.486748  \n",
       "Best Score for Training (0.95)                           0.265007  \n",
       "Best Score for Validation (0.9)                          0.268273  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean, df_median = generate_comparison(models, model_names, df_training, df_validation, training_preds, val_preds, thresh, [best_train_score, best_val_score], True)\n",
    "df_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
